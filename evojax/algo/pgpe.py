# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Implementation of the PGPE algorithm in JAX.

Ref: https://github.com/nnaisense/pgpelib/blob/release/pgpelib/pgpe.py
"""

import numpy as np
import logging
from typing import Optional
from typing import Union
from typing import Tuple
from functools import partial

import jax
import jax.numpy as jnp
from jax import random
try:
    from jax.example_libraries import optimizers
except ModuleNotFoundError:
    from jax.experimental import optimizers

from evojax.algo.base import NEAlgorithm, process_scores
from evojax.util import create_logger


@jax.jit
def compute_reinforce_update(
        fitness_scores: jnp.ndarray,
        scaled_noises: jnp.ndarray,
        stdev: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """Compute the updates for the center and the standard deviation."""

    fitness_scores = fitness_scores.reshape((-1, 2))
    baseline = jnp.mean(fitness_scores)
    all_scores = (fitness_scores[:, 0] - fitness_scores[:, 1]).squeeze()
    all_avg_scores = fitness_scores.sum(axis=-1) / 2
    stdev_sq = stdev ** 2.
    total_mu = scaled_noises * jnp.expand_dims(all_scores, axis=1) * 0.5
    total_sigma = (
            (jnp.expand_dims(all_avg_scores, axis=1) - baseline) *
            (scaled_noises ** 2 - jnp.expand_dims(stdev_sq, axis=0)) / stdev
    )
    return total_mu.mean(axis=0), total_sigma.mean(axis=0)


@jax.jit
def update_stdev(stdev: jnp.ndarray,
                 lr: float,
                 grad: jnp.ndarray,
                 max_change: float) -> jnp.ndarray:
    """Update (and clip) the standard deviation."""

    allowed_delta = jnp.abs(stdev) * max_change
    min_allowed = stdev - allowed_delta
    max_allowed = stdev + allowed_delta
    return jnp.clip(stdev + lr * grad, min_allowed, max_allowed)


@partial(jax.jit, static_argnums=(3, 4))
def ask_func(key: jnp.ndarray,
             stdev: jnp.ndarray,
             center: jnp.ndarray,
             num_directions: int,
             solution_size: int) -> Tuple[jnp.ndarray,
                                          jnp.ndarray,
                                          jnp.ndarray]:
    """A function that samples a population of parameters from Gaussian."""

    next_key, key = random.split(key)
    scaled_noises = random.normal(key, [num_directions, solution_size]) * stdev
    solutions = jnp.hstack([
        center + scaled_noises,
        center - scaled_noises]).reshape(-1, solution_size)
    return next_key, scaled_noises, solutions


class PGPE(NEAlgorithm):
    """Policy Gradient with Parameter-based Exploration (PGPE) algorithm.

    Ref: https://people.idsia.ch/~juergen/icann2008sehnke.pdf
    """

    def __init__(self,
                 pop_size: int,
                 param_size: int,
                 init_params: Optional[Union[jnp.ndarray, np.ndarray]] = None,
                 optimizer: Optional[str] = None,
                 optimizer_config: dict = None,
                 center_learning_rate: float = 0.15,
                 stdev_learning_rate: float = 0.1,
                 init_stdev: Union[float, jnp.ndarray, np.ndarray] = 0.1,
                 stdev_max_change: float = 0.2,
                 solution_ranking: bool = True,
                 seed: int = 0,
                 logger: logging.Logger = None):
        """Initialization function.

        Args:
            pop_size - Population size.
            param_size - Parameter size.
            init_params - Initial parameters, all zeros if not given.
            optimizer - Possible values are {None, 'adam', 'clipup'}.
            optimizer_config - Configurations specific to the optimizer.
                               For None: No configuration is required.
                               For Adam: {'epsilon', 'beta1', 'beta2'}.
                               For ClipUp: {'momentum', 'max_speed'}.
            center_learning_rate - Learning rate for the Gaussian mean.
            stdev_learning_rate - Learning rate for the Gaussian stdev.
            init_stdev - Initial stdev for the Gaussian distribution.
            stdev_max_change - Maximum allowed change for stdev in abs values.
            solution_ranking - Should we treat the fitness as rankings or not.
            seed - Random seed for parameters sampling.
        """

        if logger is None:
            self._logger = create_logger(name='PGPE')
        else:
            self._logger = logger

        self.pop_size = abs(pop_size)
        if self.pop_size % 2 == 1:
            self.pop_size += 1
            self._logger.info(
                'Population size should be an even number, set to {}'.format(
                    self.pop_size))
        self._num_directions = self.pop_size // 2

        if init_params is None:
            self._center = np.zeros(abs(param_size))
        else:
            self._center = init_params
        self._center = jnp.array(self._center)
        if isinstance(init_stdev, float):
            self._stdev = np.ones(abs(param_size)) * abs(init_stdev)
        self._stdev = jnp.array(self._stdev)

        self._center_lr = abs(center_learning_rate)
        self._stdev_lr = abs(stdev_learning_rate)
        self._stdev_max_change = abs(stdev_max_change)
        self._solution_ranking = solution_ranking

        if optimizer_config is None:
            optimizer_config = {}
        decay_coef = optimizer_config.get('center_lr_decay_coef', 1.)
        self._lr_decay_steps = optimizer_config.get(
            'center_lr_decay_steps', 1000)

        if optimizer == 'adam':
            opt_init, opt_update, get_params = optimizers.adam(
                step_size=lambda x: self._center_lr * jnp.power(decay_coef, x),
                b1=optimizer_config.get('beta1', 0.9),
                b2=optimizer_config.get('beta2', 0.999),
                eps=optimizer_config.get('epsilon', 1e-8),
            )
        elif optimizer == 'clipup':
            opt_init, opt_update, get_params = clipup(
                step_size=lambda x: self._center_lr * jnp.power(decay_coef, x),
                momentum=optimizer_config.get('momentum', 0.9),
                max_speed=optimizer_config.get('max_speed', 0.15),
                fix_gradient_size=optimizer_config.get(
                    'fix_gradient_size', True),
            )
        else:
            opt_init, opt_update, get_params = optimizers.sgd(
                step_size=lambda x: self._center_lr * jnp.power(decay_coef, x),
            )
        self._t = 0
        self._opt_state = jax.jit(opt_init)(self._center)
        self._opt_update = jax.jit(opt_update)
        self._get_params = jax.jit(get_params)

        self._key = random.PRNGKey(seed=seed)
        self._solutions = None
        self._scaled_noises = None

    def ask(self) -> jnp.ndarray:
        self._key, self._scaled_noises, self._solutions = ask_func(
            self._key, self._stdev, self._center,
            self._num_directions, self._center.size)
        return self._solutions

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        fitness_scores = process_scores(fitness, self._solution_ranking)
        grad_center, grad_stdev = compute_reinforce_update(
            fitness_scores=fitness_scores,
            scaled_noises=self._scaled_noises,
            stdev=self._stdev,
        )
        self._opt_state = self._opt_update(
            self._t // self._lr_decay_steps, -grad_center, self._opt_state)
        self._t += 1
        self._center = self._get_params(self._opt_state)
        self._stdev = update_stdev(
            stdev=self._stdev,
            lr=self._stdev_lr,
            max_change=self._stdev_max_change,
            grad=grad_stdev,
        )

    @property
    def best_params(self) -> jnp.ndarray:
        return jnp.array(self._center, copy=True)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        self._center = jnp.array(params, copy=True)


@optimizers.optimizer
def clipup(step_size: float,
           momentum: float = 0.9,
           max_speed: float = 0.15,
           fix_gradient_size: bool = True):
    """Construct optimizer triple for ClipUp."""

    step_size = optimizers.make_schedule(step_size)

    def init(x0):
        v0 = jnp.zeros_like(x0)
        return x0, v0

    def update(i, g, state):
        x, v = state
        g = jax.lax.cond(fix_gradient_size,
                         lambda p: p / jnp.sqrt(jnp.sum(p * p)),
                         lambda p: p,
                         g)
        step = g * step_size(i)
        v = momentum * v + step
        # Clip.
        length = jnp.sqrt(jnp.sum(v * v))
        v = jax.lax.cond(length > max_speed,
                         lambda p: p * max_speed / length,
                         lambda p: p,
                         v)
        return x - v, v

    def get_params(state):
        x, _ = state
        return x

    return init, update, get_params
